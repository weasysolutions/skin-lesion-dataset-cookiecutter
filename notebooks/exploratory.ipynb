{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *Lesiones de la piel*\n",
    "\n",
    "\n",
    "### [Aprendizaje profundo: Redes Neuronales Convolutivas con TensorFlow](https://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivación\n",
    "\n",
    "La dermatoscopia es una técnica de diagnóstico ampliamente utilizada que mejora el diagnóstico de lesiones cutáneas benignas y malignas en comparación con el examen a simple vista. Las imágenes dermatoscópicas también son una fuente adecuada para entrenar redes neuronales artificiales para diagnosticar automáticamente lesiones cutáneas pigmentadas. \n",
    "\n",
    "Los avances recientes en las capacidades de las tarjetas gráficas y las técnicas de aprendizaje en máquinas establecen nuevos puntos de referencia con respecto a la complejidad de las redes neuronales y aumentan las expectativas de que pronto estarán disponibles sistemas de diagnóstico automatizados que diagnostican todo tipo de lesiones cutáneas pigmentadas sin la necesidad de experiencia humana (https://arxiv.org/abs/1803.10417).\n",
    "\n",
    "La capacitación de algoritmos de diagnóstico basados ​​en redes neuronales requiere un gran número de imágenes anotadas (o etiquetadas), pero el número de imágenes dermatoscópicas de alta calidad con diagnósticos confiables está limitado o restringido a solo unas pocas clases de enfermedades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### El archivo ISIC (International Skin Imaging Collaboration)\n",
    "\n",
    "El archivo ISIC (https://isic-archive.com/) es una colección de múltiples bases de datos e incluye actualmente 13786 imágenes dermatoscópicas (a partír del 12 de febrero de 2018). Es la fuente estándar para la investigación de análisis de imágenes dermatoscópicas. Sin embargo, está sesgada hacia las lesiones melanocíticas (12893 de 13786 imágenes son nevi ormelanomas). \n",
    "\n",
    "Para impulsar la investigación sobre el diagnóstico automatizado de imágenes dermatoscópicas, recientemente investigadores de la Univ. Médica de Vienna y la Universidad de Queensland lanzaron el conjunto de datos HAM10000 [“Human Against Machine con 10000 imágenes de entrenamiento”]. Han colgado el conjunto de datos a través del archivo ISIC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "*El artículo original se encuentra en la siguiente liga: <br>*\n",
    "> [El conjunto de datos HAM10000: una gran colección de imágenes dermatoscópicas de múltiples fuentes de lesiones cutáneas pigmentadas comunes](https://arxiv.org/abs/1803.10417)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6f77793e7ebbcd523899bafaa9dae9763b9a0a5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Introduction y Objetivo de la Lección **\n",
    "\n",
    "Este libro detalla un proceso que construye el modelo y luego convertirlo de Keras a Tensorflow.js. El código javascript, html y css para la aplicación está disponible en github. <br>\n",
    "\n",
    "Si un modelo tiene una precisión del 60%, por lo general se consideraría un modelo malo. Sin embargo, si también tiene una precisión del 3% superior al 90% y el objetivo requiere que produzca 3 predicciones, entonces puede ser un buen modelo.\n",
    "\n",
    "*Este es el objetivo que se ha definido para esta tarea:*\n",
    "\n",
    "> Crear una herramienta en línea que pueda decirle a los médicos y tecnólogos de laboratorio los tres diagnósticos de mayor probabilidad para una lesión cutánea determinada. Esto les ayudará a identificar rápidamente a los pacientes de alta prioridad y acelerar su flujo de trabajo. La aplicación debe producir un resultado en menos de 3 segundos. Para garantizar la privacidad, las imágenes se deben preprocesar y analizar localmente y nunca se deben cargar en un servidor externo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [ Descripción de las categorías del diagnóstico:](https://arxiv.org/abs/1803.10417) <br>\n",
    "\n",
    " **nv** <br>\n",
    " Los nevos melanocíticos son neoplasias benignas de melanocitos y aparecen en una gran variedad de variantes, que están incluidas en nuestra serie. Las variantes pueden diferir significativamente desde un punto de vista dermatoscópico. <br>\n",
    " *[6705 imágenes]*\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    " **mel** <br>\n",
    " El melanoma es una neoplasia maligna derivada de melanocitos que puede aparecer en diferentes variantes. Si se extirpa en una etapa temprana, se puede curar por escisión quirúrgica simple. Los melanomas pueden ser invasivos o no invasivos (in situ). Se incluyen todas las variantes de melanoma, incluido el melanoma in situ, pero se excluye el melanoma no pigmentado, subungueal, ocular o mucoso. <br> \n",
    " *[1113 imágenes]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [ Descripción de las categorías del diagnóstico:](https://arxiv.org/abs/1803.10417) <br>\n",
    " \n",
    "** bkl ** <br>\n",
    " La \"queratosis benigna\" es una clase genérica que incluye queratosis seborreica (\"verruga senil\"), lentigo solar, que puede considerarse una variante plana de la queratosis seborreica y liquenoplus como queratosis (LPLK), que corresponde a una seborréica Queratosis o un lentigo solar con inflamación y regresión [22]. Los tres subgrupos pueden tener un aspecto dermatoscópico diferente, pero los agrupamos porque son biológicamente similares y, a menudo, se informan bajo el mismo término genérico histopatológicamente. Desde un punto de vista dermatoscópico, las queratosis de tipo liquen plano son especialmente desafiantes porque pueden mostrar características morfológicas que simulan un melanoma [23] y con frecuencia se realizan biopsias o se extirpan por razones diagnósticas.\n",
    "*[1099 imágenes]*\n",
    "***\n",
    "**bcc** <br>\n",
    "El carcinoma de células basales es una variante común del cáncer de piel epitelial que rara vez hace metástasis pero crece destructivamente si no se trata. Aparece en diferentes variantes morfológicas (plana, nodular, pigmentada, quística, etc.), todas incluidas en este conjunto. <br>\n",
    "*[514 imágenes] *\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3f6843b78793e1c047ca6909a7449dc9bfc3f1c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "** akiec ** <br>\n",
    "Las queratosis actínicas (queratosis solares) y el carcinoma intraepitelial (enfermedad de Bowen) son variantes no invasivas comunes del carcinoma de células escamosas que pueden tratarse localmente sin cirugía. Algunos autores los consideran como precursores de carcinomas de células escamosas y no como carcinomas reales. Sin embargo, hay acuerdo en que estas lesiones pueden progresar a un carcinoma de células escamosas invasivo, que generalmente no está pigmentado. Ambas neoplasias comúnmente muestran escamas en la superficie y comúnmente están desprovistas de pigmento. Las queratosis actínicas son más comunes en la cara y la enfermedad de Bowen es más común en otros sitios del cuerpo. Debido a que ambos tipos están inducidos por la luz ultravioleta, la piel circundante generalmente se caracteriza por daños severos causados ​​por el sol, excepto en los casos de enfermedad de Bowen que son causados por la infección del virus del papiloma humano y no por los rayos UV. Existen variantes pigmentadas para la enfermedad de Bowen [19] y para las queratosis actínicas [20]. Ambos están incluidos en este conjunto. <br>\n",
    "*[327 imágenes]*\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** vasc ** <br>\n",
    "Las lesiones vasculares de la piel en el conjunto de datos van desde los angiomas de cereza hasta los angioqueratomas [25] y los granulomas piógenos [26]. La hemorragia también se incluye en esta categoría. <br>\n",
    "*[142 imágenes]*\n",
    "\n",
    "** df ** <br>\n",
    "El dermatofibroma es una lesión benigna de la piel considerada como una proliferación benigna o una reacción inflamatoria a un trauma mínimo. Es marrón y con frecuencia muestra una zona central de fibrosis dermatoscópica [24]. <br>\n",
    "*[115 imágenes]*\n",
    "\n",
    "\n",
    "<br> * [Imágenes totales = 10015] *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [TensorFlow](https://www.tensorflow.org/)\n",
    "\n",
    "TensorFlow es una plataforma de código abierto  para el aprendizaje automático. Cuenta con un ecosistema integral y flexible de herramientas, bibliotecas y recursos de la comunidad que permite a los investigadores impulsar el estado de la técnica en ML y los desarrolladores pueden crear y desplegar fácilmente aplicaciones potenciadas por ML.\n",
    "\n",
    "### [Keras](https://keras.io/)\n",
    "\n",
    "Keras es una interfaz de programación de aplicaciones (API) de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre TensorFlow, CNTK o Theano. Fue desarrollado con un enfoque en permitir la experimentación rápida. Poder pasar de la idea al resultado con el menor retraso posible es clave para hacer una buena investigación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importamos todas las bibliotecas que utilizaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Generamos las semillas aleatorias iniciales\n",
    "from numpy.random import seed\n",
    "seed(101)\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "#tf.random.set_random_seed(101)\n",
    "#tf.set_random_seed(1013)\n",
    "\n",
    "# numpy para el manejo de arreglos\n",
    "\n",
    "# numpy y pandas para el manejo de los datos\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# tensorflow para el aprendizaje automatico\n",
    "#  Keras backend para las redes neuronales\n",
    "import tensorflow\n",
    "\n",
    "# Capas, Renormalizadores y Optimizadores.\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Metricas y Modelos \n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos de datos (de entrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dir>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: make_dataset.py [OPTIONS] INPUT_FILEPATH OUTPUT_FILEPATH\n",
      "\n",
      "  Runs data processing scripts to turn raw data from (../raw) into cleaned\n",
      "  data ready to be analyzed (saved in ../processed).\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!python ../src/data/make_dataset.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d5a0a200bfc57c5489eaa930255d9420a7d01c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hmnist_28_28_RGB.csv',\n",
       " 'HAM10000_images_part_1',\n",
       " 'hmnist_28_28_L.csv',\n",
       " 'HAM10000_images_part_2',\n",
       " 'hmnist_8_8_L.csv',\n",
       " 'all_images',\n",
       " 'HAM10000_metadata.csv',\n",
       " 'hmnist_8_8_RGB.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir ='../data/raw/skin-cancer-mnist-ham10000/'\n",
    "os.listdir(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "086162161ba405b800863e7d545b5917e5205984"
   },
   "source": [
    "### Crea la estructura de directorios.\n",
    "\n",
    "En estas carpetas almacenaremos las imágenes que luego se enviarán a los generadores Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploratory-Copy1.ipynb  exploratory.ipynb\n"
     ]
    }
   ],
   "source": [
    "!cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio  ../data/interim/base_dir ya existe\n"
     ]
    }
   ],
   "source": [
    "# funcion para crear directorios\n",
    "def crea_dir(nombre):\n",
    "    return print('El directorio ', nombre, 'ya existe') if os.path.isdir(nombre) else os.mkdir(nombre)\n",
    "\n",
    "# Crea un nuevo directorio \n",
    "base_dir = '../data/interim/base_dir'\n",
    "crea_dir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio  ../data/base_dir/train_dir ya existe\n",
      "El directorio  ../data/base_dir/val_dir ya existe\n"
     ]
    }
   ],
   "source": [
    "#[CREAR CARPETAS DENTRO DEL DIRECTORIO BASE]\n",
    "\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "crea_dir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "crea_dir(val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio  ../data/base_dir/train_dir/nv ya existe\n",
      "El directorio  ../data/base_dir/val_dir/nv ya existe\n",
      "El directorio  ../data/base_dir/train_dir/mel ya existe\n",
      "El directorio  ../data/base_dir/val_dir/mel ya existe\n",
      "El directorio  ../data/base_dir/train_dir/bkl ya existe\n",
      "El directorio  ../data/base_dir/val_dir/bkl ya existe\n",
      "El directorio  ../data/base_dir/train_dir/bcc ya existe\n",
      "El directorio  ../data/base_dir/val_dir/bcc ya existe\n",
      "El directorio  ../data/base_dir/train_dir/akiec ya existe\n",
      "El directorio  ../data/base_dir/val_dir/akiec ya existe\n",
      "El directorio  ../data/base_dir/train_dir/vasc ya existe\n",
      "El directorio  ../data/base_dir/val_dir/vasc ya existe\n",
      "El directorio  ../data/base_dir/train_dir/df ya existe\n",
      "El directorio  ../data/base_dir/val_dir/df ya existe\n"
     ]
    }
   ],
   "source": [
    "# Tanto  train_dir como val_dir tienen los siguientes 7 archivos:\n",
    "\n",
    "    # nv, mel, bkl, bcc, akiec, vasc, df\n",
    "\n",
    "subdirectorios = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']    \n",
    "\n",
    "for directorio in subdirectorios:\n",
    "    \n",
    "    crea_dir(os.path.join(train_dir, directorio))    \n",
    "    crea_dir(os.path.join(val_dir, directorio))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ae8d37fdee293aaffa71a79019dd7277f8288fc"
   },
   "source": [
    "#### Carga de conjunto de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "268503398ef61904e05a2c0b0667d589f08a19a8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2fdcebc22ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'HAM10000_metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv(input_dir + 'HAM10000_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### image_id es un buen indice\n",
    "##df_data[df_data['image_id'].value_counts().values>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-57f66403be4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# establezca el nuevo indice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_data' is not defined"
     ]
    }
   ],
   "source": [
    "# establezca el nuevo indice\n",
    "df_data.set_index('image_id', inplace=True)\n",
    "df_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c46ea5967e095d31dcf144b6f57f0343878fa432"
   },
   "source": [
    "#### Crea un conjunto de valores estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "53e4b7b152ed831a7d7516156ac300c0e6985ffc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>non_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ISIC_0027711</th>\n",
       "      <td>HAM_0005057</td>\n",
       "      <td>nv</td>\n",
       "      <td>histo</td>\n",
       "      <td>40.0</td>\n",
       "      <td>male</td>\n",
       "      <td>back</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0034029</th>\n",
       "      <td>HAM_0002208</td>\n",
       "      <td>nv</td>\n",
       "      <td>histo</td>\n",
       "      <td>20.0</td>\n",
       "      <td>female</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0029401</th>\n",
       "      <td>HAM_0001009</td>\n",
       "      <td>nv</td>\n",
       "      <td>consensus</td>\n",
       "      <td>35.0</td>\n",
       "      <td>male</td>\n",
       "      <td>foot</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lesion_id  dx    dx_type   age     sex     localization  \\\n",
       "image_id                                                                  \n",
       "ISIC_0027711  HAM_0005057  nv      histo  40.0    male             back   \n",
       "ISIC_0034029  HAM_0002208  nv      histo  20.0  female  lower extremity   \n",
       "ISIC_0029401  HAM_0001009  nv  consensus  35.0    male             foot   \n",
       "\n",
       "              non_duplicate  \n",
       "image_id                     \n",
       "ISIC_0027711           True  \n",
       "ISIC_0034029          False  \n",
       "ISIC_0029401           True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esto nos dirá cuántas imágenes están asociadas con cada lesion_id\n",
    "lesion_counts = df_data['lesion_id'].value_counts()\n",
    "\n",
    "# identifique las lesion_id's que tienen imágenes duplicadas y\n",
    "#las que tienen una sola imagen. Para ello, crea una nueva columna\n",
    "\n",
    "def is_unique(image):\n",
    "    return lesion_counts[image['lesion_id']] == 1\n",
    "\n",
    "df_data['non_duplicate'] = df_data.apply(is_unique , axis = 1).values\n",
    "\n",
    "df_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crea conjunto de entrenamiento que excluya las imágenes que están en el conjunto de valores¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_data[df_data['non_duplicate']]\n",
    "\n",
    "_, df_val = train_test_split(df, test_size=0.17,\n",
    "                             random_state=101, stratify=df['dx'] )\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nueva columna que etiqueta si la imagen es de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### def is_in_val(image):\n",
    "    return image.name in df_val.index\n",
    "\n",
    "df_data['is_val'] = df_data.apply(is_in_val, axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>non_duplicate</th>\n",
       "      <th>is_val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ISIC_0033340</th>\n",
       "      <td>HAM_0005887</td>\n",
       "      <td>nv</td>\n",
       "      <td>histo</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0033260</th>\n",
       "      <td>HAM_0004142</td>\n",
       "      <td>bkl</td>\n",
       "      <td>consensus</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>trunk</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0024537</th>\n",
       "      <td>HAM_0001729</td>\n",
       "      <td>mel</td>\n",
       "      <td>histo</td>\n",
       "      <td>85.0</td>\n",
       "      <td>male</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lesion_id   dx    dx_type   age   sex     localization  \\\n",
       "image_id                                                                 \n",
       "ISIC_0033340  HAM_0005887   nv      histo  60.0  male  lower extremity   \n",
       "ISIC_0033260  HAM_0004142  bkl  consensus  80.0  male            trunk   \n",
       "ISIC_0024537  HAM_0001729  mel      histo  85.0  male  upper extremity   \n",
       "\n",
       "              non_duplicate  is_val  \n",
       "image_id                             \n",
       "ISIC_0033340          False   False  \n",
       "ISIC_0033260          False   False  \n",
       "ISIC_0024537          False   False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir del conjunto de validacion elija el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "39fde25b59a9452cf700c5b2ff82cc7cc45c4a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9077, 8) (938, 8)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_data[~(df_data['is_val'])]\n",
    "df_val = df_data[(df_data['is_val'])]\n",
    "\n",
    "print(df_train.shape,df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusione los directorios de imagenes en uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/enrique/Documents/dataScience/datasets/skin-lesion/skin-cancer-mnist-ham10000/all_images'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio  /home/enrique/Documents/dataScience/datasets/skin-lesion/skin-cancer-mnist-ham10000/all_images ya existe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10015"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_files(dir_relative):\n",
    "    directory = input_dir + dir_relative\n",
    "    return list(map(lambda file: directory + file, os.listdir(directory)))\n",
    "\n",
    "                \n",
    "images = get_files('HAM10000_images_part_1/') + \\\n",
    "          get_files('HAM10000_images_part_2/') \n",
    "\n",
    "images_dir = input_dir+'all_images'\n",
    "crea_dir(images_dir)    \n",
    "\n",
    "for image in images:\n",
    "    shutil.move(image, images_dir)    \n",
    "    \n",
    "len(os.listdir(images_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mueva las imagenes a los subdirectorios de atributos que estan en los correspondientes directorios de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for image_id in df_data.index:\n",
    "    \n",
    "    image = df_data.loc[image_id]\n",
    "    directory =  val_dir if image['is_val'] else train_dir\n",
    "    \n",
    "    destino = os.path.join(directory, image['dx'], image_id + '.jpg')\n",
    "    fuente = os.path.join(images_dir,  image_id + '.jpg')\n",
    "    \n",
    "    shutil.copyfile(destino, fuente)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akiec   train: 301  val: 26\n",
      "nv   train: 5954  val: 751\n",
      "mel   train: 1074  val: 39\n",
      "vasc   train: 131  val: 11\n",
      "df   train: 109  val: 6\n",
      "bkl   train: 1024  val: 75\n",
      "bcc   train: 484  val: 30\n"
     ]
    }
   ],
   "source": [
    "# verificamos cuantos archivos hay en cada folder \n",
    "\n",
    "for sub in os.listdir(train_dir):\n",
    "    \n",
    "    num_train = len(os.listdir(os.path.join(train_dir,sub)))\n",
    "    num_val = len(os.listdir(os.path.join(val_dir,sub)))\n",
    "    \n",
    "    print( sub, '  train:', num_train, ' val:', num_val) \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "akiec   num_files: 301\n",
      "nv   num_files: 5954\n",
      "mel   num_files: 1074\n",
      "vasc   num_files: 131\n",
      "df   num_files: 109\n",
      "bkl   num_files: 1024\n",
      "bcc   num_files: 484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_dir = '../data/tmp'\n",
    "\n",
    "shutil.copytree(train_dir,tmp_dir)\n",
    "print(len(os.listdir(tmp_dir)))\n",
    "\n",
    "for subdir in os.listdir(tmp_dir):\n",
    "    \n",
    "    num_files = len(os.listdir(os.path.join(tmp_dir,subdir)))\n",
    "    \n",
    "    print( subdir, '  num_files:', num_files) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Aumento del conjunto de datos de imagen con Keras ImageDataGenerator](https://medium.com/@arindambaidya168/https-medium-com-arindambaidya168-using-keras-imagedatagenerator-b94a87cdefad)\n",
    "Las técnicas de aumento de imagen son métodos para aumentar artificialmente las variaciones de imágenes en nuestro conjunto de datos mediante el uso de giros horizontales / verticales, rotaciones, variaciones en el brillo de las imágenes, cambios horizontales / verticales, etc.\n",
    "\n",
    "#### ImageDataGenerator\n",
    "Genera lotes de datos de imágenes de tensor con aumento de datos en tiempo real. Los datos se pasarán en bucles (en lotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generador de datos de imagen\n",
    "datagen = ImageDataGenerator(\n",
    "                    rotation_range=180,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    zoom_range=0.1,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    #brightness_range=(0.9,1.1),\n",
    "                    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9077 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "lote = 50\n",
    "aug_datagen = datagen.flow_from_directory(train_dir,\n",
    "                                           save_to_dir,\n",
    "                                           save_format='jpg',\n",
    "                                           target_size=(224,224),\n",
    "                                           batch_size=lote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3e2126a39c06568a1f95da2ab42353447d1be20"
   },
   "outputs": [],
   "source": [
    "# End of Data Preparation\n",
    "### ===================================================================================== ###\n",
    "# Start of Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32dad10b7c104d2baa972da8cbadc7d6038af05c"
   },
   "source": [
    "### Set Up the Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa1041d69b0e8313324b91e3e9475799e1ad61c2"
   },
   "outputs": [],
   "source": [
    "train_path = 'base_dir/train_dir'\n",
    "valid_path = 'base_dir/val_dir'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 10\n",
    "val_batch_size = 10\n",
    "image_size = 224\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = train_dir\n",
    "validation_data_dir = val_dir\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0e5aede7139196b0d4e1344b278e7621f005550"
   },
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    tensorflow.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "train_batches = datagen.flow_from_directory(train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=train_batch_size)\n",
    "\n",
    "valid_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=val_batch_size)\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ee4ee41f1b16083bd9fc20ee9dec40acccc97dd"
   },
   "source": [
    "### Modify MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ad582cb8ea0ca2d563fc367aa89b7edfafc1a57f"
   },
   "outputs": [],
   "source": [
    "# create a copy of a mobilenet model\n",
    "\n",
    "mobile = tensorflow.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "960449ec7ecdda92ba733ad23b00b7be605f3d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenet_1.00_224\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1, 1, 1000)        0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 4,253,864\n",
      "Trainable params: 4,231,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "5b7922bdf625675834d9b63ec0e85351bd9f3c0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mobile.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f832e5865c65a013a06dbf5d500c0381020c56d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de capas de Mobile\n",
    "len(mobile.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4dd9dcf26d85a57a113e6b158cf8fceeca7f99de"
   },
   "outputs": [],
   "source": [
    "# CREATE THE MODEL ARCHITECTURE\n",
    "\n",
    "# Exclude the last 5 layers of the above model.\n",
    "# This will include all layers up to and including global_average_pooling2d_1\n",
    "x = mobile.layers[-6].output\n",
    "\n",
    "# Create a new dense layer for predictions\n",
    "# 7 corresponds to the number of classes\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n",
    "# dense layer we created above.\n",
    "\n",
    "model = Model(inputs=mobile.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b38734b72afc4289ab187a9e683cbda6bf3269bc"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "a9d74e44630c3d07a596460c8fbfda3ae7cae1e9"
   },
   "outputs": [],
   "source": [
    "# We need to choose how many layers we actually want to be trained.\n",
    "\n",
    "# Here we are freezing the weights of all layers except the\n",
    "# last 23 layers in the new model.\n",
    "# The last 23 layers of the model will be trained.\n",
    "\n",
    "for layer in model.layers[:-23]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13cf63a53e5195cb8a9725d2506c71108bc478b9"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "915f30a4f5ad369713bcb8e3bfa438219d6c8ef7"
   },
   "outputs": [],
   "source": [
    "# Define Top2 and Top3 Accuracy\n",
    "\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "def top_2_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2013ff1abae70fed845af94e7ab3d95cefad0d61"
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n",
    "              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62e7a784a33d4c868f49a3ef1f9acbc7186e3338"
   },
   "outputs": [],
   "source": [
    "# Get the labels that are associated with each index\n",
    "print(valid_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3001857c9a3c2b15c2343627e340eb1ae858fae9"
   },
   "outputs": [],
   "source": [
    "# Add weights to try to make the model more sensitive to melanoma\n",
    "\n",
    "class_weights={\n",
    "    0: 1.0, # akiec\n",
    "    1: 1.0, # bcc\n",
    "    2: 1.0, # bkl\n",
    "    3: 1.0, # df\n",
    "    4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n",
    "    5: 1.0, # nv\n",
    "    6: 1.0, # vasc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "4a5e3bc3cf44f1d4326c34ad880a302ba082e9d5",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75f62723d9dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n\u001b[0m\u001b[1;32m     13\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_batches' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = \"../data/model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                              class_weight=class_weights,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=30, verbose=1,\n",
    "                   callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3e43e3f2943db4be9d75831fe23661ae9deb44b"
   },
   "source": [
    "### Evaluate the model using the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "710ee26097924153647ac432c8ade29383fe42f1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b7a722c073da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the metric names so we can use evaulate_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# get the metric names so we can use evaulate_generator\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "68603a5e8cb5e507db95074a07b552a61fa48e11"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b724e09f6ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here the the last epoch will be used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = model.evaluate_generator(test_batches, \n\u001b[0m\u001b[1;32m      4\u001b[0m                         steps=len(df_val))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Here the the last epoch will be used.\n",
    "\n",
    "val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)\n",
    "print('val_top_2_acc:', val_top_2_acc)\n",
    "print('val_top_3_acc:', val_top_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "897f066da922d81fefa165a6b911a741c52ef7f5"
   },
   "outputs": [],
   "source": [
    "# Here the best epoch will be used.\n",
    "\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)\n",
    "print('val_top_2_acc:', val_top_2_acc)\n",
    "print('val_top_3_acc:', val_top_3_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3fffba5e0aa9088cda1865c7b8d75d72c20d0f6"
   },
   "source": [
    "### Plot the Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cbd11ef4286a751ef2918361af035d356f341ae"
   },
   "outputs": [],
   "source": [
    "# display the loss and accuracy curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_top2_acc = history.history['top_2_accuracy']\n",
    "val_top2_acc = history.history['val_top_2_accuracy']\n",
    "train_top3_acc = history.history['top_3_accuracy']\n",
    "val_top3_acc = history.history['val_top_3_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training cat acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation cat acc')\n",
    "plt.title('Training and validation cat accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\n",
    "plt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\n",
    "plt.title('Training and validation top2 accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\n",
    "plt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\n",
    "plt.title('Training and validation top3 accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4204e4056c8d12c1fee72b97912879cad4ee483f"
   },
   "source": [
    "### Create a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74a66905f7a2d702f3d2aad9abf9fe114b96f0ff"
   },
   "outputs": [],
   "source": [
    "# Get the labels of the test images.\n",
    "\n",
    "test_labels = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "53f4b22617285e923f336cdb2ffcbe1f9ff5e5db"
   },
   "outputs": [],
   "source": [
    "# We need these to plot the confusion matrix.\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5113e039e8384b96595751e084f0c5ed677080a"
   },
   "outputs": [],
   "source": [
    "# Print the label associated with each class\n",
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "701dafc5874aa60a054a74c04170cb7e8d750e94"
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "predictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dcce17ac0488ff90d29b11592c9226ed1bb210fb"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cfd9bdbbd27e27d9c5de7c6593527686445ea89"
   },
   "outputs": [],
   "source": [
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d499136cdb5fdf356515beb6e0cd1130ed584db"
   },
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "940b71bb2b37d847ba81dd67ca50c7fd5785fd35"
   },
   "outputs": [],
   "source": [
    "# argmax returns the index of the max value in a row\n",
    "cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97c6b493c368ff6565782c1bb15827f5d349ef79"
   },
   "outputs": [],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ddbd33a93468075c64ba49188a6d272a5c7828f"
   },
   "outputs": [],
   "source": [
    "# Define the labels of the class indices. These need to match the \n",
    "# order shown above.\n",
    "cm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "571f3f23d58daa03ec3fbf9207363063b2a63373"
   },
   "source": [
    "### Generate the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebc31a49c8e8ab5d1305e46a7d638e0da326da8b"
   },
   "outputs": [],
   "source": [
    "# Get the index of the class with the highest probability score\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the labels of the test images.\n",
    "y_true = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05027fe3475ab922340f9d905ec1239e2157fa05"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8313f0f4024fdb8053f943136880e63ac962ac2"
   },
   "source": [
    "**Recall** = Given a class, will the classifier be able to detect it?<br>\n",
    "**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n",
    "**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5542329b4d6891149c210b55272a981144cdf27f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c46f3f1d257241f96b4aac7eb96831ff8bbea33"
   },
   "outputs": [],
   "source": [
    "# End of Model Building\n",
    "### ===================================================================================== ###\n",
    "# Convert the Model from Keras to Tensorflow.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f9017d69bf0b84522e34841c1876b613cae1535"
   },
   "source": [
    "### Install Tensorflow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "2da93a52657b786a8eb7a0d5df6d6a2bcbd0f1c6"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a85d7889e2bada2ebe3b84fc1571a89b1a66b7b0"
   },
   "source": [
    "### Convert the model from Keras to Tensorflowjs\n",
    "The conversion code below no longer works in kaggle kernels. I've left it in for reference. \n",
    "\n",
    "In order to convert this model the workaround is as follows:<br>\n",
    "1. Recreate the model using native Keras.<br>\n",
    "2. Use the command line conversion process to convert the model from Keras to Tensorflowjs.<br>\n",
    "Here's how to do that: https://www.youtube.com/watch?v=Kc2_x6pBYGE\n",
    "\n",
    "The above steps can be done in a kaggle kernel quite easily. Tensorflowjs is still fairly new so these type of bugs are not unusual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9977179251a2feb129a946028fb74d30b9eb7341"
   },
   "outputs": [],
   "source": [
    "# create a directory to store the model files\n",
    "#os.mkdir('tfjs_dir')\n",
    "\n",
    "# convert to Tensorflow.js\n",
    "#import tensorflowjs as tfjs\n",
    "\n",
    "# Error\n",
    "# AttributeError: module 'tensorflow.python.data.ops.dataset_ops' \n",
    "    # has no attribute 'UnaryDataset'\n",
    "\n",
    "#tfjs.converters.save_keras_model(model, 'tfjs_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7df391a2792ddfb7fa2a980776aeac744612f702"
   },
   "outputs": [],
   "source": [
    "# check the the directory containing the model is available\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71dd9a6a021d4ffcc159dafd52e7f86ecc6558cb"
   },
   "outputs": [],
   "source": [
    "# view the files that make up the tensorflow.js model\n",
    "#os.listdir('tfjs_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f774cd15c6de188d4bb150f25ab600e5cbc06031"
   },
   "outputs": [],
   "source": [
    "# Delete the image data directory we created to prevent a Kaggle error.\n",
    "# Kaggle allows a max of 500 files to be saved.\n",
    "\n",
    "shutil.rmtree('base_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atributo  in os.listdir(tmp_dir):\n",
    "    \n",
    "    atributo_dir = os.path.join(tmp_dir,atributo)\n",
    "    lote = 50\n",
    "\n",
    "    # establezca como fluye las nuevas imagenes\n",
    "    aug_datagen = datagen.flow_from_directory(atributo_dir,\n",
    "                                               save_to_dir=train_dir,\n",
    "                                               save_format='jpg',\n",
    "                                               target_size=(224,224),\n",
    "                                               batch_size=lote)\n",
    "\n",
    "     # Genere las imágenes aumentadas y agréguelas a las carpetas de capacitación.\n",
    "    ######\n",
    "    num_aug_img = 6000 # Número total de imágenes en cada clase.\n",
    "    ######\n",
    "\n",
    "    num_files = len(os.listdir(atributo_dir))\n",
    "    num_batches = np.rint( (num_aug_img - num_files) / lote)\n",
    "\n",
    "    # run the generator and create about 6000 augmented images\n",
    "    for i in range(0,num_batches):\n",
    "\n",
    "        imgs, labels = next(aug_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shutil.rmtree(tmp_dir)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copia las imágenes de entrenamiento en aug_dir¶\n",
    "\n",
    "\n",
    "# note that we are not augmenting class 'nv'\n",
    "class_list = ['mel','bkl','bcc','akiec','vasc','df']\n",
    "\n",
    "for item in class_list:\n",
    "    \n",
    "    # We are creating temporary directories here because we delete these directories later\n",
    "    # create a base dir\n",
    "    \n",
    "    aug_dir = 'aug_dir'\n",
    "    os.mkdir(aug_dir)\n",
    "    # create a dir within the base dir to store images of the same class\n",
    "    img_dir = os.path.join(aug_dir, 'img_dir')\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "    # Choose a class\n",
    "    img_class = item\n",
    "\n",
    "    # list all images in that directory\n",
    "    img_list = os.listdir('base_dir/train_dir/' + img_class)\n",
    "\n",
    "    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n",
    "    for fname in img_list:\n",
    "            # source path to image\n",
    "            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n",
    "            # destination path to image\n",
    "            dst = os.path.join(img_dir, fname)\n",
    "            # copy the image from the source to the destination\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "    # point to a dir containing the images and not to the images themselves\n",
    "    path = aug_dir\n",
    "    save_path = 'base_dir/train_dir/' + img_class\n",
    "\n",
    "    # Create a data generator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        #brightness_range=(0.9,1.1),\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    aug_datagen = datagen.flow_from_directory(path,\n",
    "                                           save_to_dir=save_path,\n",
    "                                           save_format='jpg',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate the augmented images and add them to the training folders\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    num_aug_images_wanted = 6000 # total number of images we want to have in each class\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    num_files = len(os.listdir(img_dir))\n",
    "    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n",
    "\n",
    "    # run the generator and create about 6000 augmented images\n",
    "    for i in range(0,num_batches):\n",
    "\n",
    "        imgs, labels = next(aug_datagen)\n",
    "        \n",
    "    # delete temporary directory with the raw image files\n",
    "    shutil.rmtree('aug_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Para establecer las imagenes creamos un "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "aug_datagen = datagen.flow_from_directory(path,\n",
    "                                           save_to_dir=save_path,\n",
    "                                           save_format='jpg',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
